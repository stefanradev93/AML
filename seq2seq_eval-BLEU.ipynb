{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gloria/programs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Dense, Embedding, Lambda,\n",
    "                                     Bidirectional, RepeatVector, Concatenate, Dot, Softmax, GaussianDropout)\n",
    "from tensorflow.keras.layers import CuDNNLSTM as LSTM\n",
    "from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this if you want to train with attentiopn to True, otherwise False\n",
    "ATTENTION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cornell(foldername, encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    Loads the cornell movie dialogs.\n",
    "    ----------\n",
    "    INPUT:\n",
    "    foldername : string - the path to the movie dialogs (relative or absolute)\n",
    "    ----------\n",
    "    OUTPUT:\n",
    "    questions  : list - a list containing preceeding phrases\n",
    "    answers    : list - a list containing following phrases \n",
    "    \"\"\"\n",
    "    \n",
    "    # Read in dialogs\n",
    "    with open(os.path.join(foldername, 'movie_lines.txt'), 'r', encoding=encoding, errors='ignore') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    with open(os.path.join(foldername, 'movie_conversations.txt'), 'r', encoding=encoding, errors='ignore') as f:\n",
    "        ids = f.read().splitlines()\n",
    "        \n",
    "    # Create an id to line dictionary\n",
    "    split_token = ' +++$+++ '\n",
    "    id2line = {l.split(split_token)[0] : l.split(split_token)[4] for l in lines}\n",
    "    \n",
    "    # Create a list of conversations\n",
    "    conversations_ids = []\n",
    "    for conv in ids[:-1]:\n",
    "        _conv = conv.split(split_token)[-1][1:-1].replace(\"'\", '').replace(\" \", \"\")\n",
    "        conversations_ids.append(_conv.split(','))\n",
    "    \n",
    "    # Extract 'questions' and 'answers'\n",
    "    questions = [id2line[ids[i]] for ids in conversations_ids for i in range(len(ids) - 1)]\n",
    "    answers = [id2line[ids[i+1]] for ids in conversations_ids for i in range(len(ids) - 1)]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A custom sequence generator aimed at circumventing the MemoryError\n",
    "    problem caused by creating large non-sparse numpy matrices. It uses keras backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 encoder_input, \n",
    "                 decoder_input, \n",
    "                 decoder_targets, \n",
    "                 batch_size=64, \n",
    "                 n_classes=None, \n",
    "                 shuffle=True,\n",
    "                 attention=False,\n",
    "                 latent_dim=None):\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_targets = decoder_targets\n",
    "        self.n_classes = n_classes\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = np.max(self.decoder_targets) + 1\n",
    "        self.shuffle = shuffle\n",
    "        self.attention = attention\n",
    "        self.latent_dim = latent_dim\n",
    "        if self.attention and self.latent_dim is None:\n",
    "            raise ValueError('If you are using attention, generator also needs latent dim of decoder RNN!')\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(self.encoder_input.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one batch at call.\"\"\"\n",
    "        \n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Sfuffle indexes after each epoch.\"\"\"\n",
    "        \n",
    "        self.indexes = np.arange(self.encoder_input.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        \"\"\"Generates data containing the batch samples.\"\"\"\n",
    "        \n",
    "        # Simply select and one-hot encode the data\n",
    "        encoder_input_batch = self.encoder_input[indexes, :]\n",
    "        decoder_input_batch = self.decoder_input[indexes, :]\n",
    "        decoder_targets_batch = self.decoder_targets[indexes, :]\n",
    "        # If we are using attention, we also need to batch out the initial decoder states\n",
    "        if self.attention:\n",
    "            s = np.zeros((self.batch_size, self.latent_dim))\n",
    "            inputs = [encoder_input_batch, decoder_input_batch, s, s]\n",
    "        else:\n",
    "            inputs = [encoder_input_batch, decoder_input_batch]\n",
    "        outputs = to_categorical(decoder_targets_batch, num_classes=self.n_classes) \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def test_generator(self, preprocessor, n_steps=3):\n",
    "        \"\"\"A function to test if generator is meaningful.\"\"\"\n",
    "        \n",
    "        for i, batch in enumerate(self):\n",
    "            \n",
    "            (inps_batch, targets_batch) = batch\n",
    "        \n",
    "            print('*' * 10)\n",
    "            print('Batch ', i+1)\n",
    "            # If attention, we need to unpack 4 values, since the cell and hidden states are also inputs\n",
    "            if self.attention:\n",
    "                encoder_input_batch, decoder_input_batch, _, _ = inps_batch\n",
    "            else:\n",
    "                encoder_input_batch, decoder_input_batch = inps_batch\n",
    "            print('Shape of encoder_input_batch: ', encoder_input_batch.shape)\n",
    "            print('Shape of decoder_input_batch: ', decoder_input_batch.shape)\n",
    "            print('Shape of targets_batch: ', targets_batch.shape)\n",
    "\n",
    "            # Check dialogs\n",
    "            print('Dialogs: ')\n",
    "            for d in range(encoder_input_batch.shape[0]):\n",
    "                print(preprocessor.decode_input_sequence(encoder_input_batch[d]))\n",
    "                print(preprocessor.decode_output_sequence(decoder_input_batch[d]))\n",
    "                print('-' * 20)  \n",
    "\n",
    "            # Break after n steps\n",
    "            if (i+1) % n_steps == 0:\n",
    "                break\n",
    "                self.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(glove_dir, word_index, max_words, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    A function to load pre-trained embeddings and return them as a N x M numpy array.\n",
    "    --------\n",
    "    INPUT:\n",
    "    glove_dir : string  - the path (relative or absolute to the glove vector text files)\n",
    "    \n",
    "    word_index : dict   - a word - index mapping obtained by a keras Tokenizer or manually\n",
    "    \n",
    "    max_words : int     - the maximum number of words in the vocabulary used for the application\n",
    "    \n",
    "    embedding_dim : int - the dimensionality of the embedding space\n",
    "    ---------\n",
    "    OUTPUT:\n",
    "    embedding_matrix : (max_words, embedding_dim) ndarray - the matrix of embeddings which can be\n",
    "                                                            directly loaded into an Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Create a word - vec dictionary from the pre-trained glvoe vectors\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    # Now we will build an embedding matrix that we can load into an Embedding layer.\n",
    "    # It iwll be of shape (max_words, embedding_dim), where each entry i will contain\n",
    "    # the embedding for the word of index i in the reference built during tokenization.\n",
    "    # Index 0 is a placeholder!\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to bundle utility functions useful for preprocessing nlp data for a seq2seq model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len=50, vocab_size=20000):\n",
    "        \"\"\"\n",
    "        Initializes preprocessor instance.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        max_seq_len : int - defines the longest sentence allowed. If a Q or an A in a Q&A pair\n",
    "                            contains more than max_seq_len, it will be later discarded.\n",
    "                            \n",
    "        vocab_size  : int - the maximum number of words in the vocabulary. Used by keras' Tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_tokenizer = Tokenizer(num_words=vocab_size, filters='')\n",
    "        self.output_tokenizer = Tokenizer(num_words=vocab_size, filters='')\n",
    "        self.idx2word_input = {}\n",
    "        self.idx2word_output = {}\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        A custom preprocessing of the input texts. It lowers the text, replaces all short \n",
    "        verb forms with their full counterparts (I'm => I am), and removes all non-alphanumeric \n",
    "        characters as well as multiple spaces with single spaces.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        text : string - a line of text to be cleaned\n",
    "        ----------\n",
    "        OUTPUT\n",
    "        text : string - the cleaned line of text\n",
    "        \"\"\"\n",
    "        \n",
    "        # Put all lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Pronouns\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        \n",
    "        # Questions\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"why's\", \"why is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"here's\", \"here is\", text)\n",
    "        \n",
    "        # Negations\n",
    "        text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "        text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "        text = re.sub(r\"mustn't\", \"must not\", text)\n",
    "        text = re.sub(r\"haven't\", \"have not\", text)\n",
    "        text = re.sub(r\"hadn't\", \"had not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"aren't\", \"are not\", text)\n",
    "        \n",
    "        # Modals\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        \n",
    "        # Keep only alphanumeric and space\n",
    "        text = re.sub(r\"([^\\s\\w]|_)+\", \"\", text)\n",
    "        # Keep only single spaces\n",
    "        text = re.sub('\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def _add_sos(self, text):\n",
    "        return '<sos> ' + text \n",
    "\n",
    "    def _add_eos(self, text):\n",
    "        return text + ' <eos>'  \n",
    "    \n",
    "    def clean_texts(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Applies clean_text() to each input/targets pair. Adds EOS token to inputs and SOS to targets.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        inputs  - list        : a list of input sentences (strings)\n",
    "        targets - list        : a list of target sentences (strings)\n",
    "        ----------\n",
    "        OUTPUT:\n",
    "        inputs_cleaned        : list - a list of the cleaned input sentences\n",
    "        targets_cleaned       : list - a list of the cleaned target sentences used as decoder output\n",
    "        targets_input_cleaned : list - a list of the cleaned sentences used as decoder input (teacher forcing)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(inputs) == len(targets), 'Input and target length must match!'\n",
    "        \n",
    "        # Clean inputs and targets\n",
    "        inputs_cleaned = [self.clean_text(inp) for inp in inputs]\n",
    "        targets_cleaned = [self.clean_text(t) for t in targets]\n",
    "    \n",
    "        # Remove sequences larger than max sequence length\n",
    "        # First figure out indices of sentences to be removed\n",
    "        idx_to_remove = []\n",
    "        for i in range(len(inputs_cleaned)):\n",
    "            inp_len = len(inputs_cleaned[i].split())\n",
    "            target_len = len(targets_cleaned[i].split()) - 1 # we will later add eos and sos\n",
    "            if inp_len > self.max_seq_len or target_len > self.max_seq_len - 1:\n",
    "                idx_to_remove.append(i)\n",
    "        \n",
    "        # Then remove (not in-place)\n",
    "        inputs_cleaned = np.delete(inputs_cleaned, idx_to_remove)\n",
    "        targets_cleaned = np.delete(targets_cleaned, idx_to_remove)\n",
    "        \n",
    "        # Add EOS and SOS tokens, creating the input_targets list (no need for sos in decoder inputs)\n",
    "        targets_cleaned = [self._add_eos(t) for t in targets_cleaned]\n",
    "        targets_input_cleaned = [self._add_sos(t.replace('<eos>', '')) for t in targets_cleaned]\n",
    "        \n",
    "        return inputs_cleaned, targets_cleaned, targets_input_cleaned\n",
    "    \n",
    "    def tokenize_texts(self, inputs, targets, input_targets):\n",
    "        \"\"\"\n",
    "        Tokenizes the three lists of cleaned texsts using keras' Tokenizer, and padds with zeros\n",
    "        ----------\n",
    "        INPUT:\n",
    "        inputs           : list - a list of cleaned input texts (for encoder inpout)\n",
    "        targets          : list - a list of cleaned target texts with EOS (decoder output)\n",
    "        input_targets    : list - a list of cleaned target input texts wit SOS (decoder input)\n",
    "        ----------\n",
    "        OUTPUT:\n",
    "        input_sequences  : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded input sentences\n",
    "        target_sequences : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded target sentences\n",
    "        target_input_sequences : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded target input sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit tokenizers\n",
    "        self._fit_tokenizers(inputs, targets, input_targets)\n",
    "        \n",
    "        # Transform to sequences + padding\n",
    "        input_sequences = self.tokenize_input(inputs)\n",
    "        target_sequences = self.tokenize_output(targets)\n",
    "        target_input_sequences = self.tokenize_output(input_targets)\n",
    "        \n",
    "        return input_sequences, target_sequences, target_input_sequences\n",
    "        \n",
    "    def _fit_tokenizers(self, inputs, targets, input_targets):\n",
    "        \"\"\"Utility function to separate fitting from transforming texts to sequences.\"\"\"\n",
    "        \n",
    "        # Fit tokenizers\n",
    "        self.input_tokenizer.fit_on_texts(inputs)\n",
    "        self.output_tokenizer.fit_on_texts(targets + input_targets)\n",
    "        \n",
    "        # Obtain index 2 word dictionaries\n",
    "        self.idx2word_input = {v : k for k, v in self.input_tokenizer.word_index.items()}\n",
    "        self.idx2word_output = {v : k for k, v in self.output_tokenizer.word_index.items()}\n",
    "        self.word2idx_input = self.input_tokenizer.word_index\n",
    "        self.word2idx_output = self.output_tokenizer.word_index\n",
    "        \n",
    "    def tokenize_input(self, inputs):\n",
    "        return pad_sequences(self.input_tokenizer.texts_to_sequences(inputs), maxlen=self.max_seq_len)\n",
    "    \n",
    "    def tokenize_output(self, outputs):\n",
    "        return pad_sequences(self.output_tokenizer.texts_to_sequences(outputs), maxlen=self.max_seq_len, padding='post')\n",
    "        \n",
    "    def decode_input_sequence(self, sequence):\n",
    "        \"\"\"Decodes a tokenized and padded input sequence.\"\"\"\n",
    "        \n",
    "        return ' '.join([self.idx2word_input[idx] for idx in sequence if idx != 0])\n",
    "    \n",
    "    def decode_output_sequence(self, sequence):\n",
    "        \"\"\"Decodes a tokenized and padded output sequence.\"\"\"\n",
    "        \n",
    "        return ' '.join([self.idx2word_output[idx] for idx in sequence if idx != 0])\n",
    "    \n",
    "    def test_random_dialogs_raw(self, inputs, targets, num_exchanges=5):\n",
    "        \"\"\"A function to test by inspection if the raw dialogs are properly aligned.\"\"\"\n",
    "\n",
    "        indices = np.random.randint(0, len(inputs), num_exchanges)\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            print('Random dialog {}:'.format(idx))\n",
    "            print(inputs[idx])\n",
    "            print(targets[idx])\n",
    "            print('-' * 10)\n",
    "            \n",
    "    def test_random_dialogs_preprocessed(self, inputs, targets, num_exchanges=5):\n",
    "        \"\"\"A function to test by inspection if the preprocessed dialogs are properly aligned.\"\"\"\n",
    "        \n",
    "        indices = np.random.randint(0, inputs.shape[0], num_exchanges)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print('Random dialog {}:'.format(idx))\n",
    "            print(self.decode_input_sequence(inputs[idx]))\n",
    "            print(self.decode_output_sequence(targets[idx]))\n",
    "            print('-' * 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \"\"\"The ready made chat bot class utilized after training of the seq2seq model.\"\"\"\n",
    "    def __init__(self, preprocessor, training_model, encoder, decoder, attention, latent_dim=128):\n",
    "        \n",
    "        self.preprocessor = preprocessor\n",
    "        self.training_model = training_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def train_model(self, train_gen, val_gen, epochs, save_name, verbose=True):\n",
    "        \"\"\"Intereface to optimize model.\"\"\"\n",
    "        \n",
    "        # Use checkpointing\n",
    "        filepath=\"./checkpoints/weights-{}-best.hdf5\".format(save_name)\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=True, save_best_only=True, mode='max')\n",
    "        tb = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        \n",
    "        # Fit\n",
    "        h = self.training_model.fit_generator(train_gen, \n",
    "                                              validation_data=val_gen, \n",
    "                                              epochs=epochs, \n",
    "                                              verbose=verbose,\n",
    "                                              callbacks=[checkpoint,tb])\n",
    "        \n",
    "    def save_model(self, path, path_encoder, path_decoder):\n",
    "        self.training_model.save_weights(path)\n",
    "        self.encoder.save_weights(path_encoder)\n",
    "        self.decoder.save_weights(path_decoder)\n",
    "    \n",
    "    def load_model(self, path, path_encoder, path_decoder):\n",
    "        self.training_model.load_weights(path)\n",
    "        self.encoder.load_weights(path_encoder)\n",
    "        self.decoder.load_weights(path_decoder)\n",
    "    \n",
    "    def test_model(self, test_sentences, target_sentences, n_sentences=3, static=False):\n",
    "        \"\"\"Judge visually the responses of the model on test sentences (test set or custom).\"\"\"\n",
    "        \n",
    "        # Choose random indices.\n",
    "        #indices = np.random.randint(0, test_sentences.shape[0], size=n_sentences)\n",
    "        indices = np.asarray([x for x in range(n_sentences)])\n",
    "        decoded_targets = []\n",
    "        responses = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            decoded_input_sentence = self.preprocessor.decode_input_sequence(test_sentences[idx, :])\n",
    "            decoded_target_sentence = self.preprocessor.decode_output_sequence(target_sentences[idx, :])\n",
    "            \n",
    "            if self.attention:\n",
    "                response = self.answer_attention(decoded_input_sentence, static)\n",
    "            else:\n",
    "                response = self.answer(decoded_input_sentence)\n",
    "            \n",
    "            #print('Test ', i+1)\n",
    "            #print('Input sentence: ')\n",
    "            #print(decoded_input_sentence)\n",
    "            #print('Chat bot output: ')\n",
    "            #print(response)\n",
    "            #print('Expected target sentence: ')\n",
    "            #print(decoded_target_sentence)\n",
    "            #print('-' * 20)\n",
    "            decoded_targets.append(decoded_target_sentence[:-6].split(\" \")) #discard <eos>\n",
    "            responses.append(response.lower()[:-1].split(\" \")) #discard \".\"\n",
    "        return decoded_targets, responses\n",
    "    \n",
    "    def chat(self, static=False):\n",
    "        \"\"\"Chat interface.\"\"\"\n",
    "        \n",
    "        print(\"Hi, let's talk!\")\n",
    "        print(\"To end the conversation, enter an empty line. :)\")\n",
    "        while True:\n",
    "            txt = input('You: ')\n",
    "            if not txt:\n",
    "                print('Bye!')\n",
    "                break\n",
    "            if self.attention:\n",
    "                response = self.answer_attention(txt, static)\n",
    "            else:\n",
    "                response = self.answer(txt)\n",
    "            print(response)\n",
    "            \n",
    "    def answer_attention(self, question, static=False):\n",
    "        \"\"\"Answer a question with attention.\"\"\"\n",
    "        \n",
    "        text = self.preprocessor.clean_text(question)\n",
    "        encoder_output = self.encoder.predict(self.preprocessor.tokenize_input([text]))\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq[0, 0] = self.preprocessor.word2idx_output['<sos>']\n",
    "        eos_idx = self.preprocessor.word2idx_output['<eos>']\n",
    "        # Initial states\n",
    "        s, c = np.zeros((1, self.latent_dim)), np.zeros((1, self.latent_dim))\n",
    "        \n",
    "        # Decoder loop\n",
    "        output_sentence = []\n",
    "        for _ in range(self.preprocessor.max_seq_len):\n",
    "            o, s, c = self.decoder.predict([target_seq, encoder_output, s, c])\n",
    "            # Index of next word\n",
    "            #idx = np.argmax(o.flatten())\n",
    "            o_flat = o.flatten()\n",
    "            \n",
    "            if static:\n",
    "                sigma = 0 #don't add noise\n",
    "            else:\n",
    "                sigma = [0, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035]\n",
    "                sigma = np.random.choice(sigma)\n",
    "            \n",
    "            noise = np.random.normal(0,sigma,o_flat.shape[0])\n",
    "            \n",
    "            o_flat = o_flat + noise\n",
    "            \n",
    "            idx = np.argmax(o_flat)\n",
    "            \n",
    "            #apply softmax so probabilities sum to 1\n",
    "            #o_prob = np.exp(o_flat - np.max(o_flat))/(np.sum(np.exp(o_flat - np.max(o_flat))))\n",
    "            \n",
    "            #idx = [x for x in range(o_flat.shape[0])]\n",
    "            #idx = np.random.choice(idx, p=o_prob)\n",
    "            \n",
    "            #idx = np.random.multinomial(1, idx)\n",
    "            #idx = np.argmax(idx)\n",
    "            \n",
    "            #If EOS, end sentence\n",
    "            if idx == eos_idx:\n",
    "                break\n",
    "            if idx > 0:\n",
    "                word = self.preprocessor.idx2word_output[idx]\n",
    "                output_sentence.append(word)\n",
    "            # Update the decoder input which is just the word previously generated\n",
    "            target_seq[0, 0] = idx\n",
    "        # Try to guess if question or no (we coult have learnt that, just use for prettines)\n",
    "        if len(output_sentence) == 0:\n",
    "            end_char = '.'\n",
    "        else:\n",
    "            if output_sentence[0] in ['what', 'where', 'how', 'who', 'why', 'when', 'which', 'are']:\n",
    "                end_char = '?'\n",
    "            else:\n",
    "                end_char = '.'\n",
    "        # Return pretty sentence\n",
    "        return ' '.join(output_sentence).capitalize() + end_char\n",
    "        \n",
    "    def answer(self, question):\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        \n",
    "        # Clean (just in case)\n",
    "        text = self.preprocessor.clean_text(question)\n",
    "        \n",
    "        # Encode the input as state vectors.\n",
    "        states = self.encoder.predict(self.preprocessor.tokenize_input([text]))\n",
    "        \n",
    "        # Generate empty target sequence of length 1 (language generation model)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq[0, 0] = self.preprocessor.word2idx_output['<sos>']\n",
    "        eos_idx = self.preprocessor.word2idx_output['<eos>']\n",
    "        \n",
    "        # Output sequence\n",
    "        output_sentence = []\n",
    "        for _ in range(self.preprocessor.max_seq_len):\n",
    "            output_tokens, h, c = self.decoder.predict([target_seq] + states)\n",
    "            # Get next word (most probable)\n",
    "            idx = np.argmax(output_tokens[0, 0, :])\n",
    "            #idx = np.random.multinomial(20, output_tokens[0, 0, :])\n",
    "            #idx = np.argmax(idx)\n",
    "            #If EOS, end sentence\n",
    "            if idx == eos_idx:\n",
    "                break\n",
    "            if idx > 0:\n",
    "                word = self.preprocessor.idx2word_output[idx]\n",
    "                output_sentence.append(word)\n",
    "            # Update the decoder input which is just the word previously generated\n",
    "            target_seq[0, 0] = idx\n",
    "            # Update states\n",
    "            states = [h, c]\n",
    "        # Try to guess if question or no (we coult have learnt that, just use for prettines)\n",
    "        if len(output_sentence) == 0:\n",
    "            end_char = '.'\n",
    "        else:\n",
    "            if output_sentence[0] in ['what', 'where', 'how', 'who', 'why', 'when', 'which', 'are']:\n",
    "                end_char = '?'\n",
    "            else:\n",
    "                end_char = '.'\n",
    "        # Return pretty sentence\n",
    "        return ' '.join(output_sentence).capitalize() + end_char\n",
    "    \n",
    "    def _temperature(self, output_tokens):\n",
    "        \"\"\"Reweight softmax distribution.\"\"\"\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq2seq(preprocessor, lstm_dim=128, embedding_dim=100, \n",
    "                  dropout_rate=0.5, encoder_embeddings=None, decoder_embeddings=None):\n",
    "    \"\"\"Creates the seq2seq model for training and prediction.\"\"\"\n",
    "    \n",
    "    ### TRAINING\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Encoder embedding\n",
    "    encoder_embedding = Embedding(\n",
    "                    input_dim=preprocessor.vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[encoder_embeddings],\n",
    "                    input_length=preprocessor.max_seq_len,\n",
    "                    trainable=False\n",
    "    )\n",
    "    encoder_X = encoder_embedding(encoder_input)\n",
    "\n",
    "    # Encoder LSTM\n",
    "    encoder_drop = GaussianDropout(dropout_rate)(encoder_X)\n",
    "    encoder_lstm = LSTM(lstm_dim, return_state=True)\n",
    "    encoder_outputs, h, c = encoder_lstm(encoder_drop)\n",
    "\n",
    "    # \"Thought vector\"\n",
    "    encoder_states = [h, c]\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Decoder embedding\n",
    "    decoder_embedding = Embedding(\n",
    "                input_dim=preprocessor.vocab_size, \n",
    "                output_dim=embedding_dim,\n",
    "                weights=[decoder_embeddings],\n",
    "                input_length=preprocessor.max_seq_len,\n",
    "                trainable=False\n",
    "    )      \n",
    "    decoder_X = decoder_embedding(decoder_input)\n",
    "\n",
    "    # Decoder LSTM\n",
    "    decoder_drop = GaussianDropout(dropout_rate)(decoder_X)\n",
    "    decoder_lstm = LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_drop, initial_state=encoder_states)\n",
    "\n",
    "    # Softmax classifier\n",
    "    decoder_dense = Dense(preprocessor.vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Model definition and compilation\n",
    "    training_model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "    \n",
    "    ### PREDICTION MODEL\n",
    "    \n",
    "    # The encoder will output the initial decoder hidden state\n",
    "    encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(lstm_dim,))\n",
    "    decoder_state_input_c = Input(shape=(lstm_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_input_single = Input(shape=(1,))\n",
    "    decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
    "\n",
    "    # this time, we want to keep the states too, to be output\n",
    "    # by our sampling model\n",
    "    decoder_outputs, h, c = decoder_lstm(\n",
    "      decoder_input_single_x,\n",
    "      initial_state=decoder_states_inputs\n",
    "    )\n",
    "\n",
    "    decoder_states = [h, c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # The sampling model\n",
    "    decoder_model = Model(\n",
    "      [decoder_input_single] + decoder_states_inputs, \n",
    "      [decoder_outputs] + decoder_states\n",
    "    )\n",
    "\n",
    "    # Compile the training model\n",
    "    training_model.compile(\n",
    "      optimizer='rmsprop',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return training_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_seq2seq(preprocessor, lstm_dim=128, embedding_dim=100, \n",
    "                  dropout_rate=0.5, encoder_embeddings=None, decoder_embeddings=None):\n",
    "    \"\"\"Creates the seq2seq model with attention for training and prediction.\"\"\"\n",
    "    \n",
    "    \n",
    "    ### TRAINING ###\n",
    "    # -------------------------------- #\n",
    "    \n",
    "    ## ENCODER ##\n",
    "    # Encoder input\n",
    "    encoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Encoder embedding\n",
    "    encoder_embedding = Embedding(\n",
    "                    input_dim=preprocessor.vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[encoder_embeddings],\n",
    "                    input_length=preprocessor.max_seq_len,\n",
    "                    trainable=False\n",
    "    )\n",
    "    encoder_X = encoder_embedding(encoder_input)\n",
    "    \n",
    "    # Encoder is a bidirection lstm\n",
    "    encoder_drop = GaussianDropout(dropout_rate)(encoder_X)\n",
    "    encoder_lstm = Bidirectional(LSTM(lstm_dim, return_sequences=True))\n",
    "    encoder_outputs  = encoder_lstm(encoder_drop)\n",
    "    \n",
    "    ## DECODER ##\n",
    "    # Decoder input\n",
    "    decoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Decoder embedding\n",
    "    decoder_embedding = Embedding(\n",
    "                input_dim=preprocessor.vocab_size, \n",
    "                output_dim=embedding_dim,\n",
    "                weights=[decoder_embeddings],\n",
    "                input_length=preprocessor.max_seq_len,\n",
    "                trainable=False\n",
    "    )      \n",
    "    decoder_X = decoder_embedding(decoder_input)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attn_repeat_layer = RepeatVector(preprocessor.max_seq_len)\n",
    "    attn_concat_layer = Concatenate(axis=-1)\n",
    "    attn_dense1 = Dense(12, activation='tanh')\n",
    "    attn_dense2 = Dense(1)\n",
    "    attn_softmax = Softmax(axis=1)\n",
    "    attn_dot = Dot(axes=1)\n",
    "    \n",
    "    # Decoder LSTM\n",
    "    decoder_lstm = LSTM(lstm_dim, return_state=True)\n",
    "    decoder_dense = Dense(preprocessor.vocab_size, activation='softmax')\n",
    "    initial_s = Input(shape=(lstm_dim, ), name='s0') # hidden state\n",
    "    initial_c = Input(shape=(lstm_dim, ), name='c0') # cell state\n",
    "    context_last_word_concat = Concatenate(axis=2) # use for teacher forcing\n",
    "    \n",
    "    # Now we obtain the output in Ty steps\n",
    "    # In each step, we consider all encoder outputs and current s, and c\n",
    "    s = initial_s\n",
    "    c = initial_c\n",
    "    \n",
    "    outputs = []\n",
    "    # Loop for each output step\n",
    "    for t in range(preprocessor.max_seq_len):\n",
    "        \n",
    "        # Get context for step t\n",
    "        st_1 = attn_repeat_layer(s)\n",
    "        x_att = attn_concat_layer([encoder_outputs, st_1])\n",
    "        alphas = attn_dense1(x_att)\n",
    "        alphas = attn_dense2(alphas)\n",
    "        alphas = attn_softmax(alphas)\n",
    "        context = attn_dot([alphas, encoder_outputs])\n",
    "        \n",
    "        # Select previous word for teacher forcing\n",
    "        selector = Lambda(lambda x: x[:, t:t+1])\n",
    "        target_x_t = selector(decoder_X)\n",
    "        \n",
    "        # Combine context with previous word and get decoder output at step t\n",
    "        decoder_X_t = context_last_word_concat([context, target_x_t])\n",
    "        o, s, c = decoder_lstm(decoder_X_t, initial_state=[s, c])\n",
    "        \n",
    "        # Get next word prediction and add to outputs list\n",
    "        decoder_output = decoder_dense(o)\n",
    "        outputs.append(decoder_output)\n",
    "        \n",
    "    # Combine all the outputs from the list \n",
    "    # The list now contains Ty NxD matrices\n",
    "    # where N is the batch_size and D is the vocab_size\n",
    "    # Thus, we want a NxTyxD tensor => permute the stacked tensor which is TyxNxD\n",
    "    output_combiner = Lambda(lambda x: K.permute_dimensions(K.stack(x), pattern=(1, 0, 2)), name='Combiner')\n",
    "    outputs_stacked = output_combiner(outputs)\n",
    "    \n",
    "    # Create and compile the training model\n",
    "    training_model = Model(\n",
    "            inputs=[encoder_input, decoder_input, initial_s, initial_c],\n",
    "            outputs=outputs_stacked\n",
    "    )\n",
    "    \n",
    "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    ### PREDICTION ###\n",
    "\n",
    "    # Encoder\n",
    "    encoder_model = Model(encoder_input, encoder_outputs)\n",
    "    \n",
    "    # Decoder (with Ty = 1)\n",
    "    encoder_output_as_input = Input(shape=(preprocessor.max_seq_len, lstm_dim * 2, ))\n",
    "    decoder_input_single = Input(shape=(1,))\n",
    "    decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
    "    \n",
    "    # Attention\n",
    "    st_1 = attn_repeat_layer(initial_s)\n",
    "    x_att = attn_concat_layer([encoder_output_as_input, st_1])\n",
    "    alphas = attn_dense1(x_att)\n",
    "    alphas = attn_dense2(alphas)\n",
    "    alphas = attn_softmax(alphas)\n",
    "    context = attn_dot([alphas, encoder_output_as_input])\n",
    "    \n",
    "    # Decoder lstm\n",
    "    decoder_lstm_input = context_last_word_concat([context, decoder_input_single_x])\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        inputs=[decoder_input_single, encoder_output_as_input, initial_s, initial_c], \n",
    "        outputs=[decoder_outputs, s, c]\n",
    "    )\n",
    "    \n",
    "    return training_model, encoder_model, decoder_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = get_cornell('dialogs/cornell_movie-dialogs_corpus/', encoding='utf-8')\n",
    "preprocessor = Seq2SeqPreprocessor(vocab_size=20000)\n",
    "inputs_cleaned, targets_cleaned, targets_input_cleaned = preprocessor.clean_texts(questions, answers)\n",
    "input_sequences, target_sequences, target_input_sequences = preprocessor.tokenize_texts(inputs_cleaned, \n",
    "                                                                                        targets_cleaned, \n",
    "                                                                                        targets_input_cleaned)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(input_sequences_train, \n",
    "input_sequences_test, \n",
    "target_sequences_train, \n",
    "target_sequences_test, \n",
    "target_input_sequences_train, \n",
    "target_input_sequences_test) = train_test_split(input_sequences, \n",
    "                                                target_sequences, \n",
    "                                                target_input_sequences, \n",
    "                                                test_size=0.025, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embeddings = load_embeddings('embeddings', \n",
    "                                     preprocessor.word2idx_input, \n",
    "                                     preprocessor.vocab_size)\n",
    "\n",
    "\n",
    "decoder_embeddings = load_embeddings('embeddings', \n",
    "                                     preprocessor.word2idx_output, \n",
    "                                     preprocessor.vocab_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train generator\n",
    "seq_gen_train = SequenceGenerator(input_sequences_train, \n",
    "                            target_input_sequences_train, \n",
    "                            target_sequences_train, \n",
    "                            batch_size=64, \n",
    "                            n_classes=preprocessor.vocab_size,\n",
    "                            attention=ATTENTION,\n",
    "                            latent_dim=128)\n",
    "\n",
    "# Validation generator\n",
    "seq_gen_val = SequenceGenerator(input_sequences_test, \n",
    "                            target_input_sequences_test, \n",
    "                            target_sequences_test, \n",
    "                            batch_size=64, \n",
    "                            n_classes=preprocessor.vocab_size,\n",
    "                            attention=ATTENTION,\n",
    "                            latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_gen_train = SequenceGenerator(input_sequences, \n",
    "                            target_input_sequences, \n",
    "                            target_sequences, \n",
    "                            batch_size=64, \n",
    "                            n_classes=preprocessor.vocab_size,\n",
    "                            attention=ATTENTION,\n",
    "                            latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "if ATTENTION:\n",
    "    # Attention seq2seq\n",
    "    model, encoder_model, decoder_model = create_attention_seq2seq(preprocessor, \n",
    "                                                    encoder_embeddings=encoder_embeddings, \n",
    "                                                    decoder_embeddings=decoder_embeddings)\n",
    "else:\n",
    "    # Vanilla seq2seq\n",
    "    model, encoder_model, decoder_model = create_seq2seq(preprocessor, \n",
    "                                                        encoder_embeddings=encoder_embeddings, \n",
    "                                                        decoder_embeddings=decoder_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(preprocessor, model, encoder_model, decoder_model, attention=ATTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"./weights/seq2seq_attention_\"\n",
    "chatbot.load_model(prefix + \"weights_epoch_50.h5\", prefix + \"encoder_weights_epoch_50.h5\", prefix + \"decoder_weights_epoch_50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to calculate the scores on the entire dataset. BLEU compares machine generated output to \n",
    "reference \"translations\", i.e. acceptable responses. \n",
    "\"\"\"\n",
    "#non-static\n",
    "targets, responses = chatbot.test_model(input_sequences, target_sequences, n_sentences=input_sequences.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('targets.pkl', 'wb') as fp:\n",
    "    pickle.dump(targets, fp)\n",
    "\n",
    "with open('responses_non-static.pkl', 'wb') as fp:\n",
    "    pickle.dump(responses, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#load\n",
    "with open('targets.pkl', 'rb') as fp:\n",
    "    targets = pickle.load(fp)\n",
    "\n",
    "with open('responses_non-static.pkl', 'rb') as fp:\n",
    "    responses = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.859247796048789e-05\n"
     ]
    }
   ],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "score = corpus_bleu(targets, responses, smoothing_function=chencherry.method3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static\n",
    "targets_static, responses_static = chatbot.test_model(input_sequences, target_sequences, n_sentences=input_sequences.shape[0], static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('targets_static.pkl', 'wb') as fp:\n",
    "    pickle.dump(targets_static, fp)\n",
    "\n",
    "with open('responses_static.pkl', 'wb') as fp:\n",
    "    pickle.dump(responses_static, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9320963643106984e-05\n"
     ]
    }
   ],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "score = corpus_bleu(targets_static, responses_static, smoothing_function=chencherry.method3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION=False\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "if ATTENTION:\n",
    "    # Attention seq2seq\n",
    "    model, encoder_model, decoder_model = create_attention_seq2seq(preprocessor, \n",
    "                                                    encoder_embeddings=encoder_embeddings, \n",
    "                                                    decoder_embeddings=decoder_embeddings)\n",
    "else:\n",
    "    # Vanilla seq2seq\n",
    "    model, encoder_model, decoder_model = create_seq2seq(preprocessor, \n",
    "                                                        encoder_embeddings=encoder_embeddings, \n",
    "                                                        decoder_embeddings=decoder_embeddings)\n",
    "\n",
    "chatbot = ChatBot(preprocessor, model, encoder_model, decoder_model, attention=ATTENTION)\n",
    "\n",
    "prefix = \"./weights/seq2seq_basic_\"\n",
    "chatbot.load_model(prefix + \"weights_epoch_50.h5\", prefix + \"encoder_weights_epoch_50.h5\", prefix + \"decoder_weights_epoch_50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "targets_basic, responses_basic = chatbot.test_model(input_sequences, target_sequences, input_sequences.shape[0], static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('targets_basic.pkl', 'wb') as fp:\n",
    "    pickle.dump(targets_basic, fp)\n",
    "\n",
    "with open('responses_basic.pkl', 'wb') as fp:\n",
    "    pickle.dump(responses_basic, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.758398362722337e-06\n"
     ]
    }
   ],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "score = corpus_bleu(targets_basic, responses_basic, smoothing_function=chencherry.method3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
