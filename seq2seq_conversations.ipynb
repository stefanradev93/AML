{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notebook-description\" data-toc-modified-id=\"Notebook-description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notebook description</a></span></li><li><span><a href=\"#Functions-and-classes\" data-toc-modified-id=\"Functions-and-classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions and classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dialogs\" data-toc-modified-id=\"Load-dialogs-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load dialogs</a></span></li><li><span><a href=\"#Sequence-generator\" data-toc-modified-id=\"Sequence-generator-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sequence generator</a></span></li><li><span><a href=\"#Load-embeddings\" data-toc-modified-id=\"Load-embeddings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Load embeddings</a></span></li><li><span><a href=\"#Preprocessor-class\" data-toc-modified-id=\"Preprocessor-class-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Preprocessor class</a></span></li><li><span><a href=\"#Chat-bot-class\" data-toc-modified-id=\"Chat-bot-class-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Chat bot class</a></span></li><li><span><a href=\"#Vanilla-Seq2Seq\" data-toc-modified-id=\"Vanilla-Seq2Seq-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Vanilla Seq2Seq</a></span></li><li><span><a href=\"#Attention-Seq2Seq\" data-toc-modified-id=\"Attention-Seq2Seq-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Attention Seq2Seq</a></span></li></ul></li><li><span><a href=\"#Workflow\" data-toc-modified-id=\"Workflow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Workflow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-cornell-dialogs\" data-toc-modified-id=\"Loading-the-cornell-dialogs-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Loading the cornell dialogs</a></span></li><li><span><a href=\"#Preprocessing-the-dialogs\" data-toc-modified-id=\"Preprocessing-the-dialogs-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Preprocessing the dialogs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-raw-dialogs\" data-toc-modified-id=\"Inspect-raw-dialogs-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Inspect raw dialogs</a></span></li><li><span><a href=\"#Clean-texts\" data-toc-modified-id=\"Clean-texts-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Clean texts</a></span></li><li><span><a href=\"#Inspect-cleaned-dialogs\" data-toc-modified-id=\"Inspect-cleaned-dialogs-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Inspect cleaned dialogs</a></span></li><li><span><a href=\"#Tokenize-sentences\" data-toc-modified-id=\"Tokenize-sentences-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Tokenize sentences</a></span></li><li><span><a href=\"#Inspect-tokenized-dialogs\" data-toc-modified-id=\"Inspect-tokenized-dialogs-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Inspect tokenized dialogs</a></span></li><li><span><a href=\"#Load-embeddings\" data-toc-modified-id=\"Load-embeddings-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Load embeddings</a></span></li><li><span><a href=\"#Create-a-sequence-generator\" data-toc-modified-id=\"Create-a-sequence-generator-3.2.7\"><span class=\"toc-item-num\">3.2.7&nbsp;&nbsp;</span>Create a sequence generator</a></span></li><li><span><a href=\"#Test-generator\" data-toc-modified-id=\"Test-generator-3.2.8\"><span class=\"toc-item-num\">3.2.8&nbsp;&nbsp;</span>Test generator</a></span></li><li><span><a href=\"#Create-model(s)\" data-toc-modified-id=\"Create-model(s)-3.2.9\"><span class=\"toc-item-num\">3.2.9&nbsp;&nbsp;</span>Create model(s)</a></span></li><li><span><a href=\"#Train-model\" data-toc-modified-id=\"Train-model-3.2.10\"><span class=\"toc-item-num\">3.2.10&nbsp;&nbsp;</span>Train model</a></span></li><li><span><a href=\"#Chat-with-the-bot\" data-toc-modified-id=\"Chat-with-the-bot-3.2.11\"><span class=\"toc-item-num\">3.2.11&nbsp;&nbsp;</span>Chat with the bot</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "<p>This notebook deals with preprocessing the Cornell Movie Dialogs dataset and the IMDB Dialog dataset. We will need the data in a format suitable for a seq2seq model (meaning two lists of sentences).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gloria/programs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Dense, Embedding, Lambda,\n",
    "                                     Bidirectional, RepeatVector, Concatenate, Dot, Softmax, GaussianDropout)\n",
    "from tensorflow.keras.layers import CuDNNLSTM as LSTM\n",
    "from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this if you want to train with attentiopn to True, otherwise False\n",
    "ATTENTION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_cornell(foldername, encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    Loads the cornell movie dialogs.\n",
    "    ----------\n",
    "    INPUT:\n",
    "    foldername : string - the path to the movie dialogs (relative or absolute)\n",
    "    ----------\n",
    "    OUTPUT:\n",
    "    questions  : list - a list containing preceeding phrases\n",
    "    answers    : list - a list containing following phrases \n",
    "    \"\"\"\n",
    "    \n",
    "    # Read in dialogs\n",
    "    with open(os.path.join(foldername, 'movie_lines.txt'), 'r', encoding=encoding, errors='ignore') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    with open(os.path.join(foldername, 'movie_conversations.txt'), 'r', encoding=encoding, errors='ignore') as f:\n",
    "        ids = f.read().splitlines()\n",
    "        \n",
    "    # Create an id to line dictionary\n",
    "    split_token = ' +++$+++ '\n",
    "    id2line = {l.split(split_token)[0] : l.split(split_token)[4] for l in lines}\n",
    "    \n",
    "    # Create a list of conversations\n",
    "    conversations_ids = []\n",
    "    for conv in ids[:-1]:\n",
    "        _conv = conv.split(split_token)[-1][1:-1].replace(\"'\", '').replace(\" \", \"\")\n",
    "        conversations_ids.append(_conv.split(','))\n",
    "    \n",
    "    # Extract 'questions' and 'answers'\n",
    "    questions = [id2line[ids[i]] for ids in conversations_ids for i in range(len(ids) - 1)]\n",
    "    answers = [id2line[ids[i+1]] for ids in conversations_ids for i in range(len(ids) - 1)]\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_imdb(foldername):\n",
    "    \"\"\"\n",
    "    Loads the cornell movie dialogs.\n",
    "    ----------\n",
    "    INPUT:\n",
    "    foldername : string - the path to the movie dialogs (relative or absolute)\n",
    "    ----------\n",
    "    OUTPUT:\n",
    "    questions  : list - a list containing preceeding phrases\n",
    "    answers    : list - a list containing following phrases \n",
    "    \"\"\"\n",
    "    \n",
    "    questions = answers = None\n",
    "    \n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SequenceGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A custom sequence generator aimed at circumventing the MemoryError\n",
    "    problem caused by creating large non-sparse numpy matrices. It uses keras backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 encoder_input, \n",
    "                 decoder_input, \n",
    "                 decoder_targets, \n",
    "                 batch_size=64, \n",
    "                 n_classes=None, \n",
    "                 shuffle=True,\n",
    "                 attention=False,\n",
    "                 latent_dim=None):\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_targets = decoder_targets\n",
    "        self.n_classes = n_classes\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = np.max(self.decoder_targets) + 1\n",
    "        self.shuffle = shuffle\n",
    "        self.attention = attention\n",
    "        self.latent_dim = latent_dim\n",
    "        if self.attention and self.latent_dim is None:\n",
    "            raise ValueError('If you are using attention, generator also needs latent dim of decoder RNN!')\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(self.encoder_input.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one batch at call.\"\"\"\n",
    "        \n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Sfuffle indexes after each epoch.\"\"\"\n",
    "        \n",
    "        self.indexes = np.arange(self.encoder_input.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        \"\"\"Generates data containing the batch samples.\"\"\"\n",
    "        \n",
    "        # Simply select and one-hot encode the data\n",
    "        encoder_input_batch = self.encoder_input[indexes, :]\n",
    "        decoder_input_batch = self.decoder_input[indexes, :]\n",
    "        decoder_targets_batch = self.decoder_targets[indexes, :]\n",
    "        # If we are using attention, we also need to batch out the initial decoder states\n",
    "        if self.attention:\n",
    "            s = np.zeros((self.batch_size, self.latent_dim))\n",
    "            inputs = [encoder_input_batch, decoder_input_batch, s, s]\n",
    "        else:\n",
    "            inputs = [encoder_input_batch, decoder_input_batch]\n",
    "        outputs = to_categorical(decoder_targets_batch, num_classes=self.n_classes) \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def test_generator(self, preprocessor, n_steps=3):\n",
    "        \"\"\"A function to test if generator is meaningful.\"\"\"\n",
    "        \n",
    "        for i, batch in enumerate(self):\n",
    "            \n",
    "            (inps_batch, targets_batch) = batch\n",
    "        \n",
    "            print('*' * 10)\n",
    "            print('Batch ', i+1)\n",
    "            # If attention, we need to unpack 4 values, since the cell and hidden states are also inputs\n",
    "            if self.attention:\n",
    "                encoder_input_batch, decoder_input_batch, _, _ = inps_batch\n",
    "            else:\n",
    "                encoder_input_batch, decoder_input_batch = inps_batch\n",
    "            print('Shape of encoder_input_batch: ', encoder_input_batch.shape)\n",
    "            print('Shape of decoder_input_batch: ', decoder_input_batch.shape)\n",
    "            print('Shape of targets_batch: ', targets_batch.shape)\n",
    "\n",
    "            # Check dialogs\n",
    "            print('Dialogs: ')\n",
    "            for d in range(encoder_input_batch.shape[0]):\n",
    "                print(preprocessor.decode_input_sequence(encoder_input_batch[d]))\n",
    "                print(preprocessor.decode_output_sequence(decoder_input_batch[d]))\n",
    "                print('-' * 20)  \n",
    "\n",
    "            # Break after n steps\n",
    "            if (i+1) % n_steps == 0:\n",
    "                break\n",
    "                self.on_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_embeddings(glove_dir, word_index, max_words, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    A function to load pre-trained embeddings and return them as a N x M numpy array.\n",
    "    --------\n",
    "    INPUT:\n",
    "    glove_dir : string  - the path (relative or absolute to the glove vector text files)\n",
    "    \n",
    "    word_index : dict   - a word - index mapping obtained by a keras Tokenizer or manually\n",
    "    \n",
    "    max_words : int     - the maximum number of words in the vocabulary used for the application\n",
    "    \n",
    "    embedding_dim : int - the dimensionality of the embedding space\n",
    "    ---------\n",
    "    OUTPUT:\n",
    "    embedding_matrix : (max_words, embedding_dim) ndarray - the matrix of embeddings which can be\n",
    "                                                            directly loaded into an Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Create a word - vec dictionary from the pre-trained glvoe vectors\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    # Now we will build an embedding matrix that we can load into an Embedding layer.\n",
    "    # It iwll be of shape (max_words, embedding_dim), where each entry i will contain\n",
    "    # the embedding for the word of index i in the reference built during tokenization.\n",
    "    # Index 0 is a placeholder!\n",
    "    \n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     23,
     78,
     81,
     84,
     123,
     151,
     164,
     167,
     170,
     175,
     180,
     191
    ]
   },
   "outputs": [],
   "source": [
    "class Seq2SeqPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to bundle utility functions useful for preprocessing nlp data for a seq2seq model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len=50, vocab_size=20000):\n",
    "        \"\"\"\n",
    "        Initializes preprocessor instance.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        max_seq_len : int - defines the longest sentence allowed. If a Q or an A in a Q&A pair\n",
    "                            contains more than max_seq_len, it will be later discarded.\n",
    "                            \n",
    "        vocab_size  : int - the maximum number of words in the vocabulary. Used by keras' Tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_tokenizer = Tokenizer(num_words=vocab_size, filters='')\n",
    "        self.output_tokenizer = Tokenizer(num_words=vocab_size, filters='')\n",
    "        self.idx2word_input = {}\n",
    "        self.idx2word_output = {}\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        A custom preprocessing of the input texts. It lowers the text, replaces all short \n",
    "        verb forms with their full counterparts (I'm => I am), and removes all non-alphanumeric \n",
    "        characters as well as multiple spaces with single spaces.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        text : string - a line of text to be cleaned\n",
    "        ----------\n",
    "        OUTPUT\n",
    "        text : string - the cleaned line of text\n",
    "        \"\"\"\n",
    "        \n",
    "        # Put all lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Pronouns\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        \n",
    "        # Questions\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"why's\", \"why is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"here's\", \"here is\", text)\n",
    "        \n",
    "        # Negations\n",
    "        text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "        text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "        text = re.sub(r\"mustn't\", \"must not\", text)\n",
    "        text = re.sub(r\"haven't\", \"have not\", text)\n",
    "        text = re.sub(r\"hadn't\", \"had not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"aren't\", \"are not\", text)\n",
    "        \n",
    "        # Modals\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        \n",
    "        # Keep only alphanumeric and space\n",
    "        text = re.sub(r\"([^\\s\\w]|_)+\", \"\", text)\n",
    "        # Keep only single spaces\n",
    "        text = re.sub('\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def _add_sos(self, text):\n",
    "        return '<sos> ' + text \n",
    "\n",
    "    def _add_eos(self, text):\n",
    "        return text + ' <eos>'  \n",
    "    \n",
    "    def clean_texts(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Applies clean_text() to each input/targets pair. Adds EOS token to inputs and SOS to targets.\n",
    "        ----------\n",
    "        INPUT:\n",
    "        inputs  - list        : a list of input sentences (strings)\n",
    "        targets - list        : a list of target sentences (strings)\n",
    "        ----------\n",
    "        OUTPUT:\n",
    "        inputs_cleaned        : list - a list of the cleaned input sentences\n",
    "        targets_cleaned       : list - a list of the cleaned target sentences used as decoder output\n",
    "        targets_input_cleaned : list - a list of the cleaned sentences used as decoder input (teacher forcing)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(inputs) == len(targets), 'Input and target length must match!'\n",
    "        \n",
    "        # Clean inputs and targets\n",
    "        inputs_cleaned = [self.clean_text(inp) for inp in inputs]\n",
    "        targets_cleaned = [self.clean_text(t) for t in targets]\n",
    "    \n",
    "        # Remove sequences larger than max sequence length\n",
    "        # First figure out indices of sentences to be removed\n",
    "        idx_to_remove = []\n",
    "        for i in range(len(inputs_cleaned)):\n",
    "            inp_len = len(inputs_cleaned[i].split())\n",
    "            target_len = len(targets_cleaned[i].split()) - 1 # we will later add eos and sos\n",
    "            if inp_len > self.max_seq_len or target_len > self.max_seq_len - 1:\n",
    "                idx_to_remove.append(i)\n",
    "        \n",
    "        # Then remove (not in-place)\n",
    "        inputs_cleaned = np.delete(inputs_cleaned, idx_to_remove)\n",
    "        targets_cleaned = np.delete(targets_cleaned, idx_to_remove)\n",
    "        \n",
    "        # Add EOS and SOS tokens, creating the input_targets list (no need for sos in decoder inputs)\n",
    "        targets_cleaned = [self._add_eos(t) for t in targets_cleaned]\n",
    "        targets_input_cleaned = [self._add_sos(t.replace('<eos>', '')) for t in targets_cleaned]\n",
    "        \n",
    "        return inputs_cleaned, targets_cleaned, targets_input_cleaned\n",
    "    \n",
    "    def tokenize_texts(self, inputs, targets, input_targets):\n",
    "        \"\"\"\n",
    "        Tokenizes the three lists of cleaned texsts using keras' Tokenizer, and padds with zeros\n",
    "        ----------\n",
    "        INPUT:\n",
    "        inputs           : list - a list of cleaned input texts (for encoder inpout)\n",
    "        targets          : list - a list of cleaned target texts with EOS (decoder output)\n",
    "        input_targets    : list - a list of cleaned target input texts wit SOS (decoder input)\n",
    "        ----------\n",
    "        OUTPUT:\n",
    "        input_sequences  : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded input sentences\n",
    "        target_sequences : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded target sentences\n",
    "        target_input_sequences : np.ndarray - a N x max_seq_len numpy array with tokenized \n",
    "                                            and padded target input sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit tokenizers\n",
    "        self._fit_tokenizers(inputs, targets, input_targets)\n",
    "        \n",
    "        # Transform to sequences + padding\n",
    "        input_sequences = self.tokenize_input(inputs)\n",
    "        target_sequences = self.tokenize_output(targets)\n",
    "        target_input_sequences = self.tokenize_output(input_targets)\n",
    "        \n",
    "        return input_sequences, target_sequences, target_input_sequences\n",
    "        \n",
    "    def _fit_tokenizers(self, inputs, targets, input_targets):\n",
    "        \"\"\"Utility function to separate fitting from transforming texts to sequences.\"\"\"\n",
    "        \n",
    "        # Fit tokenizers\n",
    "        self.input_tokenizer.fit_on_texts(inputs)\n",
    "        self.output_tokenizer.fit_on_texts(targets + input_targets)\n",
    "        \n",
    "        # Obtain index 2 word dictionaries\n",
    "        self.idx2word_input = {v : k for k, v in self.input_tokenizer.word_index.items()}\n",
    "        self.idx2word_output = {v : k for k, v in self.output_tokenizer.word_index.items()}\n",
    "        self.word2idx_input = self.input_tokenizer.word_index\n",
    "        self.word2idx_output = self.output_tokenizer.word_index\n",
    "        \n",
    "    def tokenize_input(self, inputs):\n",
    "        return pad_sequences(self.input_tokenizer.texts_to_sequences(inputs), maxlen=self.max_seq_len)\n",
    "    \n",
    "    def tokenize_output(self, outputs):\n",
    "        return pad_sequences(self.output_tokenizer.texts_to_sequences(outputs), maxlen=self.max_seq_len, padding='post')\n",
    "        \n",
    "    def decode_input_sequence(self, sequence):\n",
    "        \"\"\"Decodes a tokenized and padded input sequence.\"\"\"\n",
    "        \n",
    "        return ' '.join([self.idx2word_input[idx] for idx in sequence if idx != 0])\n",
    "    \n",
    "    def decode_output_sequence(self, sequence):\n",
    "        \"\"\"Decodes a tokenized and padded output sequence.\"\"\"\n",
    "        \n",
    "        return ' '.join([self.idx2word_output[idx] for idx in sequence if idx != 0])\n",
    "    \n",
    "    def test_random_dialogs_raw(self, inputs, targets, num_exchanges=5):\n",
    "        \"\"\"A function to test by inspection if the raw dialogs are properly aligned.\"\"\"\n",
    "\n",
    "        indices = np.random.randint(0, len(inputs), num_exchanges)\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            print('Random dialog {}:'.format(idx))\n",
    "            print(inputs[idx])\n",
    "            print(targets[idx])\n",
    "            print('-' * 10)\n",
    "            \n",
    "    def test_random_dialogs_preprocessed(self, inputs, targets, num_exchanges=5):\n",
    "        \"\"\"A function to test by inspection if the preprocessed dialogs are properly aligned.\"\"\"\n",
    "        \n",
    "        indices = np.random.randint(0, inputs.shape[0], num_exchanges)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print('Random dialog {}:'.format(idx))\n",
    "            print(self.decode_input_sequence(inputs[idx]))\n",
    "            print(self.decode_output_sequence(targets[idx]))\n",
    "            print('-' * 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat bot class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \"\"\"The ready made chat bot class utilized after training of the seq2seq model.\"\"\"\n",
    "    def __init__(self, preprocessor, training_model, encoder, decoder, attention, latent_dim=128):\n",
    "        \n",
    "        self.preprocessor = preprocessor\n",
    "        self.training_model = training_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def train_model(self, train_gen, val_gen, epochs, save_name, verbose=True):\n",
    "        \"\"\"Intereface to optimize model.\"\"\"\n",
    "        \n",
    "        # Use checkpointing\n",
    "        filepath=\"./checkpoints/weights-{}-best.hdf5\".format(save_name)\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=True, save_best_only=True, mode='max')\n",
    "        tb = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "        \n",
    "        # Fit\n",
    "        h = self.training_model.fit_generator(train_gen, \n",
    "                                              validation_data=val_gen, \n",
    "                                              epochs=epochs, \n",
    "                                              verbose=verbose,\n",
    "                                              callbacks=[checkpoint,tb])\n",
    "        \n",
    "    def save_model(self, path, path_encoder, path_decoder):\n",
    "        self.training_model.save_weights(path)\n",
    "        self.encoder.save_weights(path_encoder)\n",
    "        self.decoder.save_weights(path_decoder)\n",
    "    \n",
    "    def load_model(self, path, path_encoder, path_decoder):\n",
    "        self.training_model.load_weights(path)\n",
    "        self.encoder.load_weights(path_encoder)\n",
    "        self.decoder.load_weights(path_decoder)\n",
    "    \n",
    "    def test_model(self, test_sentences, target_sentences, n_sentences=3, static=False):\n",
    "        \"\"\"Judge visually the responses of the model on test sentences (test set or custom).\"\"\"\n",
    "        \n",
    "        # Choose random indices.\n",
    "        #indices = np.random.randint(0, test_sentences.shape[0], size=n_sentences)\n",
    "        indices = np.asarray([x for x in range(n_sentences)])\n",
    "        decoded_targets = []\n",
    "        responses = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            decoded_input_sentence = self.preprocessor.decode_input_sequence(test_sentences[idx, :])\n",
    "            decoded_target_sentence = self.preprocessor.decode_output_sequence(target_sentences[idx, :])\n",
    "            \n",
    "            if self.attention:\n",
    "                response = self.answer_attention(decoded_input_sentence, static)\n",
    "            else:\n",
    "                response = self.answer(decoded_input_sentence)\n",
    "            \n",
    "            #print('Test ', i+1)\n",
    "            #print('Input sentence: ')\n",
    "            #print(decoded_input_sentence)\n",
    "            #print('Chat bot output: ')\n",
    "            #print(response)\n",
    "            #print('Expected target sentence: ')\n",
    "            #print(decoded_target_sentence)\n",
    "            #print('-' * 20)\n",
    "            decoded_targets.append(decoded_target_sentence[:-6].split(\" \")) #discard <eos>\n",
    "            responses.append(response.lower()[:-1].split(\" \")) #discard \".\"\n",
    "        return decoded_targets, responses\n",
    "    \n",
    "    def chat(self, static=False):\n",
    "        \"\"\"Chat interface.\"\"\"\n",
    "        \n",
    "        print(\"Hi, let's talk!\")\n",
    "        print(\"To end the conversation, enter an empty line. :)\")\n",
    "        while True:\n",
    "            txt = input('You: ')\n",
    "            if not txt:\n",
    "                print('Bye!')\n",
    "                break\n",
    "            if self.attention:\n",
    "                response = self.answer_attention(txt, static)\n",
    "            else:\n",
    "                response = self.answer(txt)\n",
    "            print(response)\n",
    "            \n",
    "    def answer_attention(self, question, static=False):\n",
    "        \"\"\"Answer a question with attention.\"\"\"\n",
    "        \n",
    "        text = self.preprocessor.clean_text(question)\n",
    "        encoder_output = self.encoder.predict(self.preprocessor.tokenize_input([text]))\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq[0, 0] = self.preprocessor.word2idx_output['<sos>']\n",
    "        eos_idx = self.preprocessor.word2idx_output['<eos>']\n",
    "        # Initial states\n",
    "        s, c = np.zeros((1, self.latent_dim)), np.zeros((1, self.latent_dim))\n",
    "        \n",
    "        # Decoder loop\n",
    "        output_sentence = []\n",
    "        for _ in range(self.preprocessor.max_seq_len):\n",
    "            o, s, c = self.decoder.predict([target_seq, encoder_output, s, c])\n",
    "            # Index of next word\n",
    "            #idx = np.argmax(o.flatten())\n",
    "            o_flat = o.flatten()\n",
    "            \n",
    "            if static:\n",
    "                sigma = 0 #don't add noise\n",
    "            else:\n",
    "                sigma = [0, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035]\n",
    "                sigma = np.random.choice(sigma)\n",
    "            \n",
    "            noise = np.random.normal(0,sigma,o_flat.shape[0])\n",
    "            \n",
    "            o_flat = o_flat + noise\n",
    "            \n",
    "            idx = np.argmax(o_flat)\n",
    "            \n",
    "            #apply softmax so probabilities sum to 1\n",
    "            #o_prob = np.exp(o_flat - np.max(o_flat))/(np.sum(np.exp(o_flat - np.max(o_flat))))\n",
    "            \n",
    "            #idx = [x for x in range(o_flat.shape[0])]\n",
    "            #idx = np.random.choice(idx, p=o_prob)\n",
    "            \n",
    "            #idx = np.random.multinomial(1, idx)\n",
    "            #idx = np.argmax(idx)\n",
    "            \n",
    "            #If EOS, end sentence\n",
    "            if idx == eos_idx:\n",
    "                break\n",
    "            if idx > 0:\n",
    "                word = self.preprocessor.idx2word_output[idx]\n",
    "                output_sentence.append(word)\n",
    "            # Update the decoder input which is just the word previously generated\n",
    "            target_seq[0, 0] = idx\n",
    "        # Try to guess if question or no (we coult have learnt that, just use for prettines)\n",
    "        if len(output_sentence) == 0:\n",
    "            end_char = '.'\n",
    "        else:\n",
    "            if output_sentence[0] in ['what', 'where', 'how', 'who', 'why', 'when', 'which', 'are']:\n",
    "                end_char = '?'\n",
    "            else:\n",
    "                end_char = '.'\n",
    "        # Return pretty sentence\n",
    "        return ' '.join(output_sentence).capitalize() + end_char\n",
    "        \n",
    "    def answer(self, question):\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        \n",
    "        # Clean (just in case)\n",
    "        text = self.preprocessor.clean_text(question)\n",
    "        \n",
    "        # Encode the input as state vectors.\n",
    "        states = self.encoder.predict(self.preprocessor.tokenize_input([text]))\n",
    "        \n",
    "        # Generate empty target sequence of length 1 (language generation model)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq[0, 0] = self.preprocessor.word2idx_output['<sos>']\n",
    "        eos_idx = self.preprocessor.word2idx_output['<eos>']\n",
    "        \n",
    "        # Output sequence\n",
    "        output_sentence = []\n",
    "        for _ in range(self.preprocessor.max_seq_len):\n",
    "            output_tokens, h, c = self.decoder.predict([target_seq] + states)\n",
    "            # Get next word (most probable)\n",
    "            idx = np.argmax(output_tokens[0, 0, :])\n",
    "            #idx = np.random.multinomial(20, output_tokens[0, 0, :])\n",
    "            #idx = np.argmax(idx)\n",
    "            #If EOS, end sentence\n",
    "            if idx == eos_idx:\n",
    "                break\n",
    "            if idx > 0:\n",
    "                word = self.preprocessor.idx2word_output[idx]\n",
    "                output_sentence.append(word)\n",
    "            # Update the decoder input which is just the word previously generated\n",
    "            target_seq[0, 0] = idx\n",
    "            # Update states\n",
    "            states = [h, c]\n",
    "        # Try to guess if question or no (we coult have learnt that, just use for prettines)\n",
    "        if len(output_sentence) == 0:\n",
    "            end_char = '.'\n",
    "        else:\n",
    "            if output_sentence[0] in ['what', 'where', 'how', 'who', 'why', 'when', 'which', 'are']:\n",
    "                end_char = '?'\n",
    "            else:\n",
    "                end_char = '.'\n",
    "        # Return pretty sentence\n",
    "        return ' '.join(output_sentence).capitalize() + end_char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_seq2seq(preprocessor, lstm_dim=128, embedding_dim=100, \n",
    "                  dropout_rate=0.5, encoder_embeddings=None, decoder_embeddings=None):\n",
    "    \"\"\"Creates the seq2seq model for training and prediction.\"\"\"\n",
    "    \n",
    "    ### TRAINING\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Encoder embedding\n",
    "    encoder_embedding = Embedding(\n",
    "                    input_dim=preprocessor.vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[encoder_embeddings],\n",
    "                    input_length=preprocessor.max_seq_len,\n",
    "                    trainable=False\n",
    "    )\n",
    "    encoder_X = encoder_embedding(encoder_input)\n",
    "\n",
    "    # Encoder LSTM\n",
    "    encoder_drop = GaussianDropout(dropout_rate)(encoder_X)\n",
    "    encoder_lstm = LSTM(lstm_dim, return_state=True)\n",
    "    encoder_outputs, h, c = encoder_lstm(encoder_drop)\n",
    "\n",
    "    # \"Thought vector\"\n",
    "    encoder_states = [h, c]\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Decoder embedding\n",
    "    decoder_embedding = Embedding(\n",
    "                input_dim=preprocessor.vocab_size, \n",
    "                output_dim=embedding_dim,\n",
    "                weights=[decoder_embeddings],\n",
    "                input_length=preprocessor.max_seq_len,\n",
    "                trainable=False\n",
    "    )      \n",
    "    decoder_X = decoder_embedding(decoder_input)\n",
    "\n",
    "    # Decoder LSTM\n",
    "    decoder_drop = GaussianDropout(dropout_rate)(decoder_X)\n",
    "    decoder_lstm = LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_drop, initial_state=encoder_states)\n",
    "\n",
    "    # Softmax classifier\n",
    "    decoder_dense = Dense(preprocessor.vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Model definition and compilation\n",
    "    training_model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "    \n",
    "    ### PREDICTION MODEL\n",
    "    \n",
    "    # The encoder will output the initial decoder hidden state\n",
    "    encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(lstm_dim,))\n",
    "    decoder_state_input_c = Input(shape=(lstm_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_input_single = Input(shape=(1,))\n",
    "    decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
    "\n",
    "    # this time, we want to keep the states too, to be output\n",
    "    # by our sampling model\n",
    "    decoder_outputs, h, c = decoder_lstm(\n",
    "      decoder_input_single_x,\n",
    "      initial_state=decoder_states_inputs\n",
    "    )\n",
    "\n",
    "    decoder_states = [h, c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # The sampling model\n",
    "    decoder_model = Model(\n",
    "      [decoder_input_single] + decoder_states_inputs, \n",
    "      [decoder_outputs] + decoder_states\n",
    "    )\n",
    "\n",
    "    # Compile the training model\n",
    "    training_model.compile(\n",
    "      optimizer='rmsprop',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return training_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_attention_seq2seq(preprocessor, lstm_dim=128, embedding_dim=100, \n",
    "                  dropout_rate=0.5, encoder_embeddings=None, decoder_embeddings=None):\n",
    "    \"\"\"Creates the seq2seq model with attention for training and prediction.\"\"\"\n",
    "    \n",
    "    \n",
    "    ### TRAINING ###\n",
    "    # -------------------------------- #\n",
    "    \n",
    "    ## ENCODER ##\n",
    "    # Encoder input\n",
    "    encoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Encoder embedding\n",
    "    encoder_embedding = Embedding(\n",
    "                    input_dim=preprocessor.vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[encoder_embeddings],\n",
    "                    input_length=preprocessor.max_seq_len,\n",
    "                    trainable=False\n",
    "    )\n",
    "    encoder_X = encoder_embedding(encoder_input)\n",
    "    \n",
    "    # Encoder is a bidirection lstm\n",
    "    encoder_drop = GaussianDropout(dropout_rate)(encoder_X)\n",
    "    encoder_lstm = Bidirectional(LSTM(lstm_dim, return_sequences=True))\n",
    "    encoder_outputs  = encoder_lstm(encoder_drop)\n",
    "    \n",
    "    ## DECODER ##\n",
    "    # Decoder input\n",
    "    decoder_input = Input(shape=(preprocessor.max_seq_len, ))\n",
    "\n",
    "    # Decoder embedding\n",
    "    decoder_embedding = Embedding(\n",
    "                input_dim=preprocessor.vocab_size, \n",
    "                output_dim=embedding_dim,\n",
    "                weights=[decoder_embeddings],\n",
    "                input_length=preprocessor.max_seq_len,\n",
    "                trainable=False\n",
    "    )      \n",
    "    decoder_X = decoder_embedding(decoder_input)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attn_repeat_layer = RepeatVector(preprocessor.max_seq_len)\n",
    "    attn_concat_layer = Concatenate(axis=-1)\n",
    "    attn_dense1 = Dense(12, activation='tanh')\n",
    "    attn_dense2 = Dense(1)\n",
    "    attn_softmax = Softmax(axis=1)\n",
    "    attn_dot = Dot(axes=1)\n",
    "    \n",
    "    # Decoder LSTM\n",
    "    decoder_lstm = LSTM(lstm_dim, return_state=True)\n",
    "    decoder_dense = Dense(preprocessor.vocab_size, activation='softmax')\n",
    "    initial_s = Input(shape=(lstm_dim, ), name='s0') # hidden state\n",
    "    initial_c = Input(shape=(lstm_dim, ), name='c0') # cell state\n",
    "    context_last_word_concat = Concatenate(axis=2) # use for teacher forcing\n",
    "    \n",
    "    # Now we obtain the output in Ty steps\n",
    "    # In each step, we consider all encoder outputs and current s, and c\n",
    "    s = initial_s\n",
    "    c = initial_c\n",
    "    \n",
    "    outputs = []\n",
    "    # Loop for each output step\n",
    "    for t in range(preprocessor.max_seq_len):\n",
    "        \n",
    "        # Get context for step t\n",
    "        st_1 = attn_repeat_layer(s)\n",
    "        x_att = attn_concat_layer([encoder_outputs, st_1])\n",
    "        alphas = attn_dense1(x_att)\n",
    "        alphas = attn_dense2(alphas)\n",
    "        alphas = attn_softmax(alphas)\n",
    "        context = attn_dot([alphas, encoder_outputs])\n",
    "        \n",
    "        # Select previous word for teacher forcing\n",
    "        selector = Lambda(lambda x: x[:, t:t+1])\n",
    "        target_x_t = selector(decoder_X)\n",
    "        \n",
    "        # Combine context with previous word and get decoder output at step t\n",
    "        decoder_X_t = context_last_word_concat([context, target_x_t])\n",
    "        o, s, c = decoder_lstm(decoder_X_t, initial_state=[s, c])\n",
    "        \n",
    "        # Get next word prediction and add to outputs list\n",
    "        decoder_output = decoder_dense(o)\n",
    "        outputs.append(decoder_output)\n",
    "        \n",
    "    # Combine all the outputs from the list \n",
    "    # The list now contains Ty NxD matrices\n",
    "    # where N is the batch_size and D is the vocab_size\n",
    "    # Thus, we want a NxTyxD tensor => permute the stacked tensor which is TyxNxD\n",
    "    output_combiner = Lambda(lambda x: K.permute_dimensions(K.stack(x), pattern=(1, 0, 2)), name='Combiner')\n",
    "    outputs_stacked = output_combiner(outputs)\n",
    "    \n",
    "    # Create and compile the training model\n",
    "    training_model = Model(\n",
    "            inputs=[encoder_input, decoder_input, initial_s, initial_c],\n",
    "            outputs=outputs_stacked\n",
    "    )\n",
    "    \n",
    "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    ### PREDICTION ###\n",
    "\n",
    "    # Encoder\n",
    "    encoder_model = Model(encoder_input, encoder_outputs)\n",
    "    \n",
    "    # Decoder (with Ty = 1)\n",
    "    encoder_output_as_input = Input(shape=(preprocessor.max_seq_len, lstm_dim * 2, ))\n",
    "    decoder_input_single = Input(shape=(1,))\n",
    "    decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
    "    \n",
    "    # Attention\n",
    "    st_1 = attn_repeat_layer(initial_s)\n",
    "    x_att = attn_concat_layer([encoder_output_as_input, st_1])\n",
    "    alphas = attn_dense1(x_att)\n",
    "    alphas = attn_dense2(alphas)\n",
    "    alphas = attn_softmax(alphas)\n",
    "    context = attn_dot([alphas, encoder_output_as_input])\n",
    "    \n",
    "    # Decoder lstm\n",
    "    decoder_lstm_input = context_last_word_concat([context, decoder_input_single_x])\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        inputs=[decoder_input_single, encoder_output_as_input, initial_s, initial_c], \n",
    "        outputs=[decoder_outputs, s, c]\n",
    "    )\n",
    "    \n",
    "    return training_model, encoder_model, decoder_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the cornell dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = get_cornell('dialogs/cornell_movie-dialogs_corpus/', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Seq2SeqPreprocessor(vocab_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect raw dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dialog 99812:\n",
      "Sorry to disturb you, chief but we have new developments.\n",
      "Oh? What?\n",
      "----------\n",
      "Random dialog 125827:\n",
      "You wanted to meet here?\n",
      "Me?  You called it.  I got a message that...\n",
      "----------\n",
      "Random dialog 1238:\n",
      "CONINUED\n",
      "Well... it's silly, but... if you want to, why don't you?\n",
      "----------\n",
      "Random dialog 160101:\n",
      "We can't leave him!\n",
      "We have to!\n",
      "----------\n",
      "Random dialog 8966:\n",
      "Those boys, those sad, raging boys... They came to me as the needy do. And like many of the needy, they were rude. Like all the needy, they took. And like all the needy, they needed.  Father. I knew them; They learn in our school. And play in our schoolyard. And they are good boys.\n",
      "You knew them?  Who were they, Sister? Who are these boys? What are the names of these -- good boys you knew?\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "preprocessor.test_random_dialogs_raw(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_cleaned, targets_cleaned, targets_input_cleaned = preprocessor.clean_texts(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect cleaned dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dialog 134482:\n",
      "it is michael how are you mom\n",
      "i am alright will you stay home for awhile <eos>\n",
      "----------\n",
      "Random dialog 89740:\n",
      "we have to find the dog ryan\n",
      "why <eos>\n",
      "----------\n",
      "Random dialog 107253:\n",
      "dyle or whatever his name is\n",
      "what does your mr dyle look like mrs lampert <eos>\n",
      "----------\n",
      "Random dialog 195694:\n",
      "what is that\n",
      "the thing that started all of this <eos>\n",
      "----------\n",
      "Random dialog 42741:\n",
      "you have stromwell\n",
      "did she do that to you too <eos>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "preprocessor.test_random_dialogs_raw(inputs_cleaned, targets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences, target_sequences, target_input_sequences = preprocessor.tokenize_texts(inputs_cleaned, \n",
    "                                                                                        targets_cleaned, \n",
    "                                                                                        targets_input_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214903, 50)\n",
      "(214903, 50)\n",
      "(214903, 50)\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences.shape)\n",
    "print(target_sequences.shape)\n",
    "print(target_input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some splits as validation\n",
    "(input_sequences_train, \n",
    "input_sequences_test, \n",
    "target_sequences_train, \n",
    "target_sequences_test, \n",
    "target_input_sequences_train, \n",
    "target_input_sequences_test) = train_test_split(input_sequences, \n",
    "                                                target_sequences, \n",
    "                                                target_input_sequences, \n",
    "                                                test_size=0.025, \n",
    "                                                random_state=42)\n",
    "\n",
    "#(input_sequences_test,\n",
    "#input_sequences_dev,\n",
    "#target_sequences_test, \n",
    "#target_sequences_dev, \n",
    "#target_input_sequences_test, \n",
    "#target_input_sequences_dev) = train_test_split(input_sequences_test, \n",
    "#                                                target_sequences_test, \n",
    "#                                                target_input_sequences_test, \n",
    "#                                                test_size=0.5, \n",
    "#                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_sequences_train:  (209530, 50)\n",
      "Shape of target_sequences_train:  (209530, 50)\n",
      "Shape of target_input_sequences_train:  (209530, 50)\n",
      "--------------------\n",
      "Shape of input_sequences_test:  (5373, 50)\n",
      "Shape of target_sequences_test:  (5373, 50)\n",
      "Shape of target_input_sequences_test:  (5373, 50)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Shape of input_sequences_train: ', input_sequences_train.shape)\n",
    "print('Shape of target_sequences_train: ', target_sequences_train.shape)\n",
    "print('Shape of target_input_sequences_train: ', target_input_sequences_train.shape)\n",
    "print('-' * 20)\n",
    "print('Shape of input_sequences_test: ', input_sequences_test.shape)\n",
    "print('Shape of target_sequences_test: ', target_sequences_test.shape)\n",
    "print('Shape of target_input_sequences_test: ', target_input_sequences_test.shape)\n",
    "print('-' * 20)\n",
    "#print('Shape of target_sequences_dev: ', target_sequences_dev.shape)\n",
    "#print('Shape of input_sequences_dev: ', input_sequences_dev.shape)\n",
    "#print('Shape of target_input_sequences_dev: ', target_input_sequences_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect tokenized dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dialog 132427:\n",
      "just like proper english gentlemen i am proud of them\n",
      "they are boys and they are indian <eos>\n",
      "----------\n",
      "Random dialog 181940:\n",
      "really well that makes two of us\n",
      "what <eos>\n",
      "----------\n",
      "Random dialog 149143:\n",
      "look this is nothing personal but i do not think you can do it\n",
      "thelma i have lost my wife i am not losing my child <eos>\n",
      "----------\n",
      "Random dialog 113899:\n",
      "what i talk like a cop this is the way i talk i cannot believe this guy saks he is a deputy assistant director of the fbi let me help you\n",
      "we could use a little help <eos>\n",
      "----------\n",
      "Random dialog 83101:\n",
      "that is a big trunk it fits a tuba a suitcase a dead dog and a bag almost perfectly\n",
      "that is just what they used to say in the ads come on crabtree i know you are holding <eos>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "preprocessor.test_random_dialogs_preprocessed(input_sequences, target_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder embeddings:  (20000, 100)\n",
      "Shape of decoder embeddings:  (20000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load separately, since we might have different words in input, output\n",
    "encoder_embeddings = load_embeddings('embeddings', \n",
    "                                     preprocessor.word2idx_input, \n",
    "                                     preprocessor.vocab_size)\n",
    "\n",
    "\n",
    "decoder_embeddings = load_embeddings('embeddings', \n",
    "                                     preprocessor.word2idx_output, \n",
    "                                     preprocessor.vocab_size)\n",
    "\n",
    "print('Shape of encoder embeddings: ', encoder_embeddings.shape)\n",
    "print('Shape of decoder embeddings: ', decoder_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators for batch sampling\n",
    "\n",
    "# Train generator\n",
    "seq_gen_train = SequenceGenerator(input_sequences_train, \n",
    "                            target_input_sequences_train, \n",
    "                            target_sequences_train, \n",
    "                            batch_size=64, \n",
    "                            n_classes=preprocessor.vocab_size,\n",
    "                            attention=ATTENTION,\n",
    "                            latent_dim=128)\n",
    "\n",
    "# Validation generator\n",
    "seq_gen_val = SequenceGenerator(input_sequences_test, \n",
    "                            target_input_sequences_test, \n",
    "                            target_sequences_test, \n",
    "                            batch_size=64, \n",
    "                            n_classes=preprocessor.vocab_size,\n",
    "                            attention=ATTENTION,\n",
    "                            latent_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conversational abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION = False\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "if ATTENTION:\n",
    "    # Attention seq2seq\n",
    "    model, encoder_model, decoder_model = create_attention_seq2seq(preprocessor, \n",
    "                                                    encoder_embeddings=encoder_embeddings, \n",
    "                                                    decoder_embeddings=decoder_embeddings)\n",
    "else:\n",
    "    # Vanilla seq2seq\n",
    "    model, encoder_model, decoder_model = create_seq2seq(preprocessor, \n",
    "                                                        encoder_embeddings=encoder_embeddings, \n",
    "                                                        decoder_embeddings=decoder_embeddings)\n",
    "\n",
    "chatbot = ChatBot(preprocessor, model, encoder_model, decoder_model, attention=ATTENTION)\n",
    "\n",
    "prefix = \"./weights/seq2seq_basic_\"\n",
    "chatbot.load_model(prefix + \"weights_epoch_50.h5\", prefix + \"encoder_weights_epoch_50.h5\", prefix + \"decoder_weights_epoch_50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, let's talk!\n",
      "To end the conversation, enter an empty line. :)\n",
      "You: Hi!\n",
      "Hi.\n",
      "You: Hello.\n",
      "I am sorry.\n",
      "You: Hey.\n",
      "I am sorry.\n",
      "You: How are you?\n",
      "I am not going to be.\n",
      "You: Where are you?\n",
      "I am not going to be.\n",
      "You: Who are you?\n",
      "I am not going to be.\n",
      "You: Are you a boy?\n",
      "No.\n",
      "You: Do you have hobbies?\n",
      "I am not going to be.\n",
      "You: Don't do that!\n",
      "I do not know what i am going to do.\n",
      "You: Stop!\n",
      "I am not going to be.\n",
      "You: Goodbye.\n",
      "I am not going to be.\n",
      "You: See you!\n",
      "I am not going to be.\n",
      "You: Bye!\n",
      "I am sorry.\n",
      "You: \n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "chatbot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "if ATTENTION:\n",
    "    # Attention seq2seq\n",
    "    model, encoder_model, decoder_model = create_attention_seq2seq(preprocessor, \n",
    "                                                    encoder_embeddings=encoder_embeddings, \n",
    "                                                    decoder_embeddings=decoder_embeddings)\n",
    "else:\n",
    "    # Basic seq2seq\n",
    "    model, encoder_model, decoder_model = create_seq2seq(preprocessor, \n",
    "                                                        encoder_embeddings=encoder_embeddings, \n",
    "                                                        decoder_embeddings=decoder_embeddings)\n",
    "\n",
    "chatbot = ChatBot(preprocessor, model, encoder_model, decoder_model, attention=ATTENTION)\n",
    "\n",
    "prefix = \"./weights/seq2seq_attention_\"\n",
    "chatbot.load_model(prefix + \"weights_epoch_50.h5\", prefix + \"encoder_weights_epoch_50.h5\", prefix + \"decoder_weights_epoch_50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With static responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, let's talk!\n",
      "To end the conversation, enter an empty line. :)\n",
      "You: Hi!\n",
      "Hi.\n",
      "You: Hello.\n",
      "Hello.\n",
      "You: Hey.\n",
      "Hi.\n",
      "You: How are you?\n",
      "Fine.\n",
      "You: Where are you?\n",
      "I am going to have to go home.\n",
      "You: Who are you?\n",
      "I am not.\n",
      "You: Are you a boy?\n",
      "No.\n",
      "You: Do you have hobbies?\n",
      "I do not know.\n",
      "You: Don't do that!\n",
      "I am not going to be a little girl.\n",
      "You: Stop!\n",
      "I am not going to be a little bit of a little girl.\n",
      "You: Goodbye.\n",
      "I am not going to be.\n",
      "You: See you!\n",
      "I am not going to be.\n",
      "You: Bye!\n",
      "I am sorry.\n",
      "You: \n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "chatbot.chat(static=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With non-static responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, let's talk!\n",
      "To end the conversation, enter an empty line. :)\n",
      "You: Hi!\n",
      "Hi.\n",
      "You: Hi!\n",
      "Hi.\n",
      "You: Hello.\n",
      "Hello.\n",
      "You: Hello.\n",
      "Hello.\n",
      "You: Hey.\n",
      "Embarrass me.\n",
      "You: Hey.\n",
      "Hey.\n",
      "You: How are you?\n",
      "Fine.\n",
      "You: How are you?\n",
      "Fine.\n",
      "You: Where are you?\n",
      "I am going to do royal.\n",
      "You: Where are you?\n",
      "I am going to zimm.\n",
      "You: Who are you?\n",
      "I am not.\n",
      "You: Who are you?\n",
      "I am not going to be.\n",
      "You: Are you a boy?\n",
      "No.\n",
      "You: Are you a boy?\n",
      "No.\n",
      "You: Do you have hobbies?\n",
      "No.\n",
      "You: Do you have hobbies?\n",
      "I do not know.\n",
      "You: Don't do that!\n",
      "I am not going to be galloping.\n",
      "You: Don't do that!\n",
      "I am not going to be a assets.\n",
      "You: Stop!\n",
      "I am not going to be.\n",
      "You: Stop!\n",
      "I am not going to be a little frenchman.\n",
      "You: Goodbye.\n",
      "I am not going to be.\n",
      "You: Goodbye.\n",
      "I am not going to be.\n",
      "You: See you!\n",
      "Mohammed expect me to without the sticky.\n",
      "You: See you!\n",
      "Cracks.\n",
      "You: Bye!\n",
      "Eightyfour.\n",
      "You: Bye!\n",
      "I am jerrys.\n",
      "You: \n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "759px",
    "left": "79px",
    "top": "96.5667px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
